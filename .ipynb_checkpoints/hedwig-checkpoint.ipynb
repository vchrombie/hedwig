{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 10.0.1 from /home/vchrombie/anaconda3/lib/python3.7/site-packages/pip (python 3.7)\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/fb/863012b13912709c13cf5cfdbfb304fa6c727659d6290438e1a88df9d848/pip-19.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 210kB/s ta 0:00:01\n",
      "\u001b[31mgrimoirelab-toolkit 0.1.9 has requirement python-dateutil>=2.8.0, but you'll have python-dateutil 2.7.3 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.1\n",
      "Requirement already satisfied: nltk in /home/vchrombie/anaconda3/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied: six in /home/vchrombie/anaconda3/lib/python3.7/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: singledispatch in /home/vchrombie/anaconda3/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip -V\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\" Tokenize tweets \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop_words = text.ENGLISH_STOP_WORDS\n",
    "    temp = data\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    temp = regex.sub(' ', temp)\n",
    "    temp = \"\".join(b for b in temp if ord(b) < 128)\n",
    "    temp = temp.lower()\n",
    "    words = temp.split()\n",
    "    no_stop_words = [w for w in words if not w in stop_words]\n",
    "    stemmed = [stemmer.stem(item) for item in no_stop_words]\n",
    "\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_location(data):\n",
    "    \"\"\" Reduces dataset to only contain tweets with location\n",
    "        marked as either WA or MA \"\"\"\n",
    "    temp = data[(data.location.str.contains(r'[.]+ WA$'))\n",
    "            | (data.location.str.contains(r'[.]+ MA$'))\n",
    "            | (data.location.str.contains('Boston'))\n",
    "            | (data.location.str.contains('Seattle'))\n",
    "            | (data.location.str.contains(r'[.]+ Washington\\s'))\n",
    "            | (data.location.str.contains('Massachusetts'))]\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_locations(data):\n",
    "    \"\"\" Creates target variables 1: for WA and 0: MA \"\"\"\n",
    "    targets = []\n",
    "    for location in data.location.apply(lambda x: x.encode('utf-8').strip()):\n",
    "        if (r'[.]+ WA$' in location) or ('Seattle' in location) or (r'[.]+ Washington\\s' in location):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_datasets(data, targets):\n",
    "    \"\"\" Balances datasets by selecting random points from\n",
    "        the minority class \"\"\"\n",
    "    new_data = data.copy()\n",
    "    if (len(targets[targets==0])) > (len(targets[targets==1])):\n",
    "        points_needed = len(targets[targets==0]) - len(targets[targets==1])\n",
    "        indices = np.where(targets == 1)\n",
    "    else:\n",
    "        points_needed = len(targets[targets==1]) - len(targets[targets==0])\n",
    "        indices = np.where(targets == 0)\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    indices = np.resize(indices, points_needed)\n",
    "    new_data = new_data.append(data.iloc[indices])\n",
    "    targets_to_add = targets[indices]\n",
    "    new_targets = np.concatenate([targets, targets_to_add])\n",
    "    return new_data, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'tweet_data/'\n",
    "filename = 'tweets_#superbowl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tweets from superbowl\n",
    "tweets_ = []\n",
    "with open(filename, 'r') as f:\n",
    "    for row in f:\n",
    "        jrow = json.loads(row)\n",
    "        d = {\n",
    "            'tweet': jrow['title'],\n",
    "            'location': jrow['tweet']['user']['location']\n",
    "        }\n",
    "        tweets_.append(d)\n",
    "all_data = pd.DataFrame(tweets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tweets by appropriate location data\n",
    "reduced_data = reduce_by_location(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target label\n",
    "# 0: MA 1: WA\n",
    "all_targets = map_locations(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance datset\n",
    "data, train_targets = balance_datasets(reduced_data, all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorize tweets\n",
    "vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=tokenize)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_counts = vectorizer.fit_transform(data.tweet)\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate twitter data to 50 features\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_reduced = svd.fit_transform(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling For Certain Algorithms Require Nonnegative Values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_data = min_max_scaler.fit_transform(train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naivebayes():\n",
    "    # Perform 5-Fold CV to fit Naive Bayes Model\n",
    "\n",
    "    accuracies = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        X_train, X_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "        clf = MultinomialNB().fit(X_train, y_train)\n",
    "        predicted_bayes = clf.predict(X_test)\n",
    "        accuracy_bayes = np.mean(predicted_bayes == y_test)\n",
    "        accuracies += accuracy_bayes\n",
    "\n",
    "    print \"Average CV-Accuracy of Multinomial Naive Bayes: \" + str(accuracies/k)\n",
    "    print(classification_report(y_test, predicted_bayes))\n",
    "    print \"Confusion Matrix:\"\n",
    "    print(confusion_matrix(y_test, predicted_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logreg():\n",
    "    # Perform 5-Fold CV to fit Logistic Regression\n",
    "    \n",
    "    accuracies = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        X_train, X_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "        logit = sk.LogisticRegression().fit(X_train, y_train)\n",
    "        probabilities = logit.predict(X_test)\n",
    "        predicted_lr = (probabilities > 0.5).astype(int)\n",
    "        accuracy_lr = np.mean(predicted_lr == y_test)\n",
    "        accuracies += accuracy_lr\n",
    "\n",
    "    print \"Average CV-Accuracy of Logistic Regression: \" + str(accuracies/k)\n",
    "    print(classification_report(y_test, predicted_lr))\n",
    "    print \"Confusion Matrix:\"\n",
    "    print(confusion_matrix(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svm():\n",
    "    # Perform 5-Fold CV to fit Logistic Regression\n",
    "    \n",
    "    accuracies = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        X_train, X_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "        linear_SVM = LinearSVC(dual=False, random_state=42).fit(X_train, y_train)\n",
    "        predicted_svm = linear_SVM.predict(X_test)\n",
    "        accuracy_svm = np.mean(predicted_svm == y_test)\n",
    "        accuracies += accuracy_svm\n",
    "\n",
    "    print \"Average CV-Accuracy of Linear SVM: \" + str(accuracies/k)\n",
    "    print(classification_report(y_test, predicted_svm))\n",
    "    print \"Confusion Matrix:\"\n",
    "    print(confusion_matrix(y_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Comparisions are shown below\\n\\n\")\n",
    "test_naivebayes()\n",
    "test_logreg()\n",
    "test_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
